{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport time\nimport cv2\nfrom sklearn import metrics\nimport gc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-29T00:41:53.319589Z","iopub.execute_input":"2023-06-29T00:41:53.320032Z","iopub.status.idle":"2023-06-29T00:41:53.335673Z","shell.execute_reply.started":"2023-06-29T00:41:53.319995Z","shell.execute_reply":"2023-06-29T00:41:53.334304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lung Cancer Prediction\n\n## We will import a dataset on lung cancer and use it to make prediction models using LogisticRegression and d RandomForestClassifier. This will aim to correlate factors that may increase ones' risk of lung cancer. We need to clean and prepare the dataset, we will explore data with visuals, and then implement the models. We will print predictions and accuracy scores to help validate the model. ","metadata":{}},{"cell_type":"code","source":"# First we need to read in dataset\ndata = pd.read_csv(\"/kaggle/input/cancer-patients-and-air-pollution-a-new-link/cancer patient data sets.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.338638Z","iopub.execute_input":"2023-06-29T00:41:53.339158Z","iopub.status.idle":"2023-06-29T00:41:53.351878Z","shell.execute_reply.started":"2023-06-29T00:41:53.339117Z","shell.execute_reply":"2023-06-29T00:41:53.350807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.353362Z","iopub.execute_input":"2023-06-29T00:41:53.353853Z","iopub.status.idle":"2023-06-29T00:41:53.367434Z","shell.execute_reply.started":"2023-06-29T00:41:53.353825Z","shell.execute_reply":"2023-06-29T00:41:53.366444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.370096Z","iopub.execute_input":"2023-06-29T00:41:53.370418Z","iopub.status.idle":"2023-06-29T00:41:53.390108Z","shell.execute_reply.started":"2023-06-29T00:41:53.370392Z","shell.execute_reply":"2023-06-29T00:41:53.388975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now that our data is here, let us do some data cleaning and preprocessing.","metadata":{}},{"cell_type":"code","source":"#look for and remove any null values\n\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.391476Z","iopub.execute_input":"2023-06-29T00:41:53.391780Z","iopub.status.idle":"2023-06-29T00:41:53.407049Z","shell.execute_reply.started":"2023-06-29T00:41:53.391755Z","shell.execute_reply":"2023-06-29T00:41:53.405856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look for unique values\ndata.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.410455Z","iopub.execute_input":"2023-06-29T00:41:53.410796Z","iopub.status.idle":"2023-06-29T00:41:53.424682Z","shell.execute_reply.started":"2023-06-29T00:41:53.410767Z","shell.execute_reply":"2023-06-29T00:41:53.423688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us drop the columns of index and patient id because they are not needed in this analysis\ndata.drop(['index', 'Patient Id'], inplace = True, axis = 1)\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.426008Z","iopub.execute_input":"2023-06-29T00:41:53.426924Z","iopub.status.idle":"2023-06-29T00:41:53.447556Z","shell.execute_reply.started":"2023-06-29T00:41:53.426884Z","shell.execute_reply":"2023-06-29T00:41:53.446401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let us change the level column from words to number so we can do EDA\n\ndata = data.replace({'Level': {'Low': 1, 'Medium': 2, 'High': 3}})\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.449262Z","iopub.execute_input":"2023-06-29T00:41:53.449708Z","iopub.status.idle":"2023-06-29T00:41:53.473279Z","shell.execute_reply.started":"2023-06-29T00:41:53.449670Z","shell.execute_reply":"2023-06-29T00:41:53.472374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now we can make a few visuals","metadata":{}},{"cell_type":"code","source":"# Let us make a pairplot to show some correlations, but we won't use all at the same time\n# as the dataset is large, but we can focus on different chosen factors easily\nsns.pairplot(data, vars = ['Gender', 'Genetic Risk', 'Air Pollution', 'Obesity'], hue = \"Age\", markers = [\"o\", \"s\"]) ","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:41:53.476700Z","iopub.execute_input":"2023-06-29T00:41:53.477051Z","iopub.status.idle":"2023-06-29T00:42:21.066976Z","shell.execute_reply.started":"2023-06-29T00:41:53.477013Z","shell.execute_reply":"2023-06-29T00:42:21.065773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a heatmap is a much more reasonable way to look at all of the data \n\nplt.figure(figsize = (20, 10))\nsns.heatmap(data.corr(), annot = True, linewidth = .5)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:21.068378Z","iopub.execute_input":"2023-06-29T00:42:21.068755Z","iopub.status.idle":"2023-06-29T00:42:23.426516Z","shell.execute_reply.started":"2023-06-29T00:42:21.068723Z","shell.execute_reply":"2023-06-29T00:42:23.425379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next, we want to split our datset and make a training model","metadata":{}},{"cell_type":"code","source":"# we will use this as our new variables\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.428148Z","iopub.execute_input":"2023-06-29T00:42:23.428520Z","iopub.status.idle":"2023-06-29T00:42:23.434375Z","shell.execute_reply.started":"2023-06-29T00:42:23.428489Z","shell.execute_reply":"2023-06-29T00:42:23.433539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will make the test train split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.435407Z","iopub.execute_input":"2023-06-29T00:42:23.436326Z","iopub.status.idle":"2023-06-29T00:42:23.452311Z","shell.execute_reply.started":"2023-06-29T00:42:23.436294Z","shell.execute_reply":"2023-06-29T00:42:23.451103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us fit the model\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.454487Z","iopub.execute_input":"2023-06-29T00:42:23.454894Z","iopub.status.idle":"2023-06-29T00:42:23.466677Z","shell.execute_reply.started":"2023-06-29T00:42:23.454861Z","shell.execute_reply":"2023-06-29T00:42:23.465607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the training set\nX_train","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.467976Z","iopub.execute_input":"2023-06-29T00:42:23.468420Z","iopub.status.idle":"2023-06-29T00:42:23.481660Z","shell.execute_reply.started":"2023-06-29T00:42:23.468381Z","shell.execute_reply":"2023-06-29T00:42:23.480417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the test set\nX_test","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.482919Z","iopub.execute_input":"2023-06-29T00:42:23.483277Z","iopub.status.idle":"2023-06-29T00:42:23.494825Z","shell.execute_reply.started":"2023-06-29T00:42:23.483250Z","shell.execute_reply":"2023-06-29T00:42:23.494116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We will look at the accuracy score of the Logistic Regression function and look at a confusion matrix","metadata":{}},{"cell_type":"code","source":"# Let us fit it to the Logistic Regression function\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.495642Z","iopub.execute_input":"2023-06-29T00:42:23.495926Z","iopub.status.idle":"2023-06-29T00:42:23.533351Z","shell.execute_reply.started":"2023-06-29T00:42:23.495902Z","shell.execute_reply":"2023-06-29T00:42:23.532087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us make predictions\n\ny_predictions = clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.535121Z","iopub.execute_input":"2023-06-29T00:42:23.535815Z","iopub.status.idle":"2023-06-29T00:42:23.541028Z","shell.execute_reply.started":"2023-06-29T00:42:23.535774Z","shell.execute_reply":"2023-06-29T00:42:23.540120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can make a confusion matrix and get an accuracy score \n\nmatrix = confusion_matrix(y_test, y_predictions)\nprint(matrix)\naccuracy_score(y_test, y_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.542263Z","iopub.execute_input":"2023-06-29T00:42:23.542561Z","iopub.status.idle":"2023-06-29T00:42:23.558652Z","shell.execute_reply.started":"2023-06-29T00:42:23.542537Z","shell.execute_reply":"2023-06-29T00:42:23.557431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can visually represent the confusion matrix as well \nsns.heatmap(matrix, annot = True)\nplt.ylabel('Prediction')\nplt.xlabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.560265Z","iopub.execute_input":"2023-06-29T00:42:23.560705Z","iopub.status.idle":"2023-06-29T00:42:23.965071Z","shell.execute_reply.started":"2023-06-29T00:42:23.560664Z","shell.execute_reply":"2023-06-29T00:42:23.963919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next, we will try the data in a RandomForestClassifier and see our confusion matrix and accuracy score","metadata":{}},{"cell_type":"code","source":"# Let us make a random forest classifier\n\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:23.966704Z","iopub.execute_input":"2023-06-29T00:42:23.967858Z","iopub.status.idle":"2023-06-29T00:42:24.176213Z","shell.execute_reply.started":"2023-06-29T00:42:23.967815Z","shell.execute_reply":"2023-06-29T00:42:24.175112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us make the predictions\ny_predictions = rfc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:24.177436Z","iopub.execute_input":"2023-06-29T00:42:24.177773Z","iopub.status.idle":"2023-06-29T00:42:24.202070Z","shell.execute_reply.started":"2023-06-29T00:42:24.177745Z","shell.execute_reply":"2023-06-29T00:42:24.200930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc_matrix = confusion_matrix(y_test, y_predictions)\nprint(rfc_matrix)\naccuracy_score(y_test, y_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:24.203494Z","iopub.execute_input":"2023-06-29T00:42:24.203931Z","iopub.status.idle":"2023-06-29T00:42:24.215556Z","shell.execute_reply.started":"2023-06-29T00:42:24.203896Z","shell.execute_reply":"2023-06-29T00:42:24.214574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can visually represent the confusion matrix as well \nsns.heatmap(rfc_matrix, annot = True)\nplt.ylabel('Prediction')\nplt.xlabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T00:42:24.216825Z","iopub.execute_input":"2023-06-29T00:42:24.217960Z","iopub.status.idle":"2023-06-29T00:42:24.627808Z","shell.execute_reply.started":"2023-06-29T00:42:24.217927Z","shell.execute_reply":"2023-06-29T00:42:24.626599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In conclusion, both models seem like they will be able to accurately predict lung cancer based on this dataset. We could remove other variables that would be key indicators to make it seemingly harder to detect or determine detection amongst the levels with different variables. You could also use images in another study for detection. ","metadata":{}}]}